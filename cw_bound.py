# script to analyze the perturbations generated by the C&W_L2 attack and bound them with a fix L2 norm.
import torch
import save_load_interface
import numpy as np
import Dataset
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import torch.nn.functional as F 

 
adversaries_file_path = ""   # path of the .pth file generated by AttackEcocModel.py with a CW_L2 attack 
metrics_file_path = ""       # path of the .npz file generated by AttackEcocModel.py with a CW_L2 attack

seed = 42
nb_images = 2000     
dataset_name = "Fashion-MNIST" # "CIFAR10"  # dataset name "Fashion-MNIST" or "CIFAR10"
threshold = 1.5     # L2 threshold (1.0 for CIFAR10 and 1.5 for Fashion-MNIST)

def main(): 
    print("\n Evaluating Ecoc model" ) 
    print(adversaries_file_path)

    # check if cuda available 
    print("CUDA available: {}\n".format(torch.cuda.is_available()))
    device = torch.device("cuda" if(torch.cuda.is_available()) else "cpu") 
   
    adv_examples_checkpoint = save_load_interface.load_adversarial_examples(adversaries_file_path) 
    test_set_subset = adv_examples_checkpoint['dataLoader']
    test_set_subset = DataLoader(test_set_subset.dataset, batch_size=1) # reinitialize the dataset to set the proper batch size.

    # Load clean images with same seed used for the adversarial attack 
    _, test_dataset = Dataset.getDataset(dataset_name)
    test_data = Dataset.get_test_subset(test_dataset, 1, nb_images, seed=seed)

    # load metrics data from file 
    data = np.load(metrics_file_path)

    true_labels = data['true_labels']
    predicted_labels_clean = data['pred_labels_clean']
    predicted_labels = data['predicted_labels']

    model_attack_score = data['acc_attack']
    print("Model score against attack when unbounded: " + str(model_attack_score))

    count = 0
    l2_dist = [] 
    score = []
    for (img, _), (img_clean, _) in zip(test_set_subset, test_data): 
        pred_label = int(predicted_labels[count])
        true_label = int(true_labels[count])
        pred_clean_label = int(predicted_labels_clean[count])

        if true_label == pred_label: 
            score.append(0) 
        elif true_label != pred_label and true_label == pred_clean_label: 
            score.append(1) 
        elif true_label != pred_label and true_label != pred_clean_label: 
            score.append(2)

        # compute L2 distance between original and perturbed images 
        l2 = torch.dist(img_clean.detach().to("cpu"), img.detach().to("cpu")).item()
        l2_dist.append(l2) 
        count += 1

    score = np.array(score) 
    l2_dist = np.array(l2_dist)
    
    #get list of l2_dist for images that fool the network and are classified correctly in their clean form 
    l2_bad = l2_dist[score == 1]
    t = l2_bad[l2_bad>threshold]
    print("Model score against attack when bounded : {}".format(t.shape[0] / nb_images))

if __name__ == "__main__": 
    main() 